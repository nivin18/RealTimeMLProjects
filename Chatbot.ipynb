{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqD1HhEvh5uRmB+sFDH2Tx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nivin18/RealTimeMLProjects/blob/master/Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyEifSCT_Lsb",
        "outputId": "2b666573-f33d-4b51-aaac-23164ceb3a3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "#Libraries needed for NLP\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "#Libraries needed for Tensorflow processing\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load the intents.json file from your local device\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "goXil1Kt_3O2",
        "outputId": "7b405662-8f19-4eff-d637-c2b2b775d696"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6e7b0666-5fd6-4bc7-8d35-b9525e2ae2ed\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6e7b0666-5fd6-4bc7-8d35-b9525e2ae2ed\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving intents.json to intents.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'intents.json': b'{\\r\\n        \"intents\": [\\r\\n                {\\r\\n                        \"tag\": \"greeting\",\\r\\n                        \"patterns\": [\\r\\n                                \"Hi\",\\r\\n                                \"How are you\",\\r\\n                                \"Is anyone there?\",\\r\\n                                \"Hello\",\\r\\n                                \"Good day\"\\r\\n                        ],\\r\\n                        \"responses\": [\\r\\n                                \"Hello, thanks for visiting\",\\r\\n                                \"Good to see you again\",\\r\\n                                \"Hi there, how can I help?\"\\r\\n                        ],\\r\\n                        \"context_set\": \"\"\\r\\n                },\\r\\n                {\\r\\n                        \"tag\": \"goodbye\",\\r\\n                        \"patterns\": [\\r\\n                                \"Bye\",\\r\\n                                \"See you later\",\\r\\n                                \"Goodbye\"\\r\\n                        ],\\r\\n                        \"responses\": [\\r\\n                                \"See you later, thanks for visiting\",\\r\\n                                \"Have a nice day\",\\r\\n                                \"Bye! Come back again soon.\"\\r\\n                        ]\\r\\n                },\\r\\n                {\\r\\n                        \"tag\": \"thanks\",\\r\\n                        \"patterns\": [\\r\\n                                \"Thanks\",\\r\\n                                \"Thank you\",\\r\\n                                \"That\\'s helpful\"\\r\\n                        ],\\r\\n                        \"responses\": [\\r\\n                                \"Happy to help!\",\\r\\n                                \"Any time!\",\\r\\n                                \"My pleasure\"\\r\\n                        ]\\r\\n                },\\r\\n                {\\r\\n                        \"tag\": \"chatbot\",\\r\\n                        \"patterns\": [\\r\\n                                \"Who built this chatbot?\",\\r\\n                                \"Tell me about Chatbot\",\\r\\n                                \"What is this chatbot name?\"\\r\\n                        ],\\r\\n                        \"responses\": [\\r\\n                                \"Hi, I am Chatbot designed by Nivin.\",\\r\\n                                \"Thanks for asking. I am designed by Nivin A Thomas.\",\\r\\n                                \"I am a chatbot.\"\\r\\n                        ]\\r\\n                },\\r\\n                {\\r\\n                        \"tag\": \"location\",\\r\\n                        \"patterns\": [\\r\\n                                \"What is your location?\",\\r\\n                                \"Where are you located?\",\\r\\n                                \"What is your address?\"\\r\\n                        ],\\r\\n                        \"responses\": [\\r\\n                                \"We are from World\\'s largest Democracy India.\",\\r\\n                                \"You can visit India to meet us\",\\r\\n                                \"Thans for your Interest. I live in India.\"\\r\\n                        ]\\r\\n                },\\r\\n                {\\r\\n                        \"tag\": \"connect\",\\r\\n                        \"patterns\": [\\r\\n                                \"Give me your social media accounts link\",\\r\\n                                \"Where can we connect\",\\r\\n                                \"How can i reach out to you?\",\\r\\n                                \"Is there any way we can connect\"\\r\\n                        ],\\r\\n                        \"responses\": [\\r\\n                                \"You can connect me on Linkedin  \\\\n Linkedin - https://www.linkedin.com/in/nivin-abraham-thomas-2033b3195/\"\\r\\n\\r\\n                        ]\\r\\n                },\\r\\n                {\\r\\n                        \"tag\": \"movies\",\\r\\n                        \"patterns\": [\\r\\n                                \"Which is your favourite movie?\",\\r\\n                                \"Suggest me some movies\",\\r\\n                                \"Recommend movies\"\\r\\n                        ],\\r\\n                        \"responses\": [\\r\\n                                \"3 idiots\",\\r\\n                                \"Hera Pheri\",\\r\\n                                \"Lage Raho Munna Bhai\",\\r\\n                                \"OMG: Oh My God!\",\\r\\n                                \"PK\",\\r\\n                                \"Yeh Jawani hai Deewani\",\\r\\n                                \"Zindagi na Milegi Doobara\",\\r\\n                                \"Ludo\"\\r\\n                        ]\\r\\n                },\\r\\n                {\\r\\n                        \"tag\": \"about\",\\r\\n                        \"patterns\": [\\r\\n                                \"Who are you?\",\\r\\n                                \"Tell me about Yourself\",\\r\\n                                \"What is this?\"\\r\\n                        ],\\r\\n                        \"responses\": [\\r\\n                                \"Hi, I am Nivin. Nice to meet you. I made this chatbot for fun and practice.\",\\r\\n                                \"Thanks for asking. I am Nivin, coder by profession but ML enthusiast by passion.\"\\r\\n                        ]\\r\\n                }\\r\\n               \\r\\n        ]\\r\\n}'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import our chat-bot intents file\n",
        "with open('intents.json') as json_data:\n",
        "  intents = json.load(json_data)"
      ],
      "metadata": {
        "id": "NXkvt6jKAZM-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intents\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njAjRU7pA2fS",
        "outputId": "69b381b7-cc67-48d1-8fe8-d3f555c0fbab"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'intents': [{'tag': 'greeting',\n",
              "   'patterns': ['Hi', 'How are you', 'Is anyone there?', 'Hello', 'Good day'],\n",
              "   'responses': ['Hello, thanks for visiting',\n",
              "    'Good to see you again',\n",
              "    'Hi there, how can I help?'],\n",
              "   'context_set': ''},\n",
              "  {'tag': 'goodbye',\n",
              "   'patterns': ['Bye', 'See you later', 'Goodbye'],\n",
              "   'responses': ['See you later, thanks for visiting',\n",
              "    'Have a nice day',\n",
              "    'Bye! Come back again soon.']},\n",
              "  {'tag': 'thanks',\n",
              "   'patterns': ['Thanks', 'Thank you', \"That's helpful\"],\n",
              "   'responses': ['Happy to help!', 'Any time!', 'My pleasure']},\n",
              "  {'tag': 'chatbot',\n",
              "   'patterns': ['Who built this chatbot?',\n",
              "    'Tell me about Chatbot',\n",
              "    'What is this chatbot name?'],\n",
              "   'responses': ['Hi, I am Chatbot designed by Nivin.',\n",
              "    'Thanks for asking. I am designed by Nivin A Thomas.',\n",
              "    'I am a chatbot.']},\n",
              "  {'tag': 'location',\n",
              "   'patterns': ['What is your location?',\n",
              "    'Where are you located?',\n",
              "    'What is your address?'],\n",
              "   'responses': [\"We are from World's largest Democracy India.\",\n",
              "    'You can visit India to meet us',\n",
              "    'Thans for your Interest. I live in India.']},\n",
              "  {'tag': 'connect',\n",
              "   'patterns': ['Give me your social media accounts link',\n",
              "    'Where can we connect',\n",
              "    'How can i reach out to you?',\n",
              "    'Is there any way we can connect'],\n",
              "   'responses': ['You can connect me on Linkedin  \\n Linkedin - https://www.linkedin.com/in/nivin-abraham-thomas-2033b3195/']},\n",
              "  {'tag': 'movies',\n",
              "   'patterns': ['Which is your favourite movie?',\n",
              "    'Suggest me some movies',\n",
              "    'Recommend movies'],\n",
              "   'responses': ['3 idiots',\n",
              "    'Hera Pheri',\n",
              "    'Lage Raho Munna Bhai',\n",
              "    'OMG: Oh My God!',\n",
              "    'PK',\n",
              "    'Yeh Jawani hai Deewani',\n",
              "    'Zindagi na Milegi Doobara',\n",
              "    'Ludo']},\n",
              "  {'tag': 'about',\n",
              "   'patterns': ['Who are you?', 'Tell me about Yourself', 'What is this?'],\n",
              "   'responses': ['Hi, I am Nivin. Nice to meet you. I made this chatbot for fun and practice.',\n",
              "    'Thanks for asking. I am Nivin, coder by profession but ML enthusiast by passion.']}]}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore = ['?']\n",
        "# loop through each sentence in the intent's patterns\n",
        "for intent in intents['intents']:\n",
        "  for pattern in intent['patterns']:\n",
        "    # tokenize each and every word in the sentence\n",
        "    w = nltk.word_tokenize(pattern)\n",
        "    # add word to the words list\n",
        "    words.extend(w)\n",
        "    # add word(s) to documents\n",
        "    documents.append((w, intent['tag']))\n",
        "    # add tags to our classes list\n",
        "    if intent['tag'] not in classes:\n",
        "      classes.append(intent['tag'])"
      ],
      "metadata": {
        "id": "npL9qozdBarb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform stemming and lower each word as well as remove duplicate\n",
        "words = [stemmer.stem(w.lower()) for w in words if w not in ignore]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "# remove duplicate classes\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "print(len(documents), \"documents\")\n",
        "print(len(classes), \"classes\", classes)\n",
        "print(len(words), \"unique stemmed words\", words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HO79cVueIp8Z",
        "outputId": "ccaae095-f143-48ce-d527-4d5e2347f0fd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27 documents\n",
            "8 classes ['about', 'chatbot', 'connect', 'goodbye', 'greeting', 'location', 'movies', 'thanks']\n",
            "52 unique stemmed words [\"'s\", 'about', 'account', 'address', 'ani', 'anyon', 'are', 'built', 'bye', 'can', 'chatbot', 'connect', 'day', 'favourit', 'give', 'good', 'goodby', 'hello', 'help', 'hi', 'how', 'i', 'is', 'later', 'link', 'locat', 'me', 'media', 'movi', 'name', 'out', 'reach', 'recommend', 'see', 'social', 'some', 'suggest', 'tell', 'thank', 'that', 'there', 'thi', 'to', 'way', 'we', 'what', 'where', 'which', 'who', 'you', 'your', 'yourself']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create training data\n",
        "training = []\n",
        "output = []\n",
        "# create an empty array for output\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "# create training set, bag of words for each sentence\n",
        "for doc in documents:\n",
        "  #initialize bag of words\n",
        "  bag = []\n",
        "  # list of tokenized words for the pattern\n",
        "  pattern_words = doc[0]\n",
        "  # stemming each words\n",
        "  pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
        "  # create bag of words array\n",
        "  for w in words:\n",
        "    bag.append(1) if w in pattern_words else bag.append(0)\n",
        "\n",
        "    # output is '1' for current tag and '0' for the rest of other tags\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "\n",
        "    training.append([bag, output_row])\n",
        "\n",
        "#shuffling features and turning it into np.array\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "\n",
        "#creating training lists\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVQPlJvGlWD1",
        "outputId": "fbf2bdce-2556-4292-dae1-2afa0b2cbcc7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-69abb7a2978d>:27: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  training = np.array(training)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(10,input_shape=(len(train_x[0]),)))\n",
        "model.add(tf.keras.layers.Dense(10))\n",
        "model.add(tf.keras.layers.Dense(len(train_y[0]), activation='softmax'))\n",
        "model.compile(tf.keras.optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "G9hVjDXXuHie"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(np.array(train_x), np.array(train_y), epochs=250, batch_size=8, verbose=1)\n",
        "model.save(\"model.pk1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAmNU8xEwd_R",
        "outputId": "d24fcc1a-472e-47b7-d9c9-cd4bd85fcb9e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "176/176 [==============================] - 1s 2ms/step - loss: 1.6962 - accuracy: 0.4281\n",
            "Epoch 2/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 0.8111 - accuracy: 0.9530\n",
            "Epoch 3/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 0.2520 - accuracy: 1.0000\n",
            "Epoch 4/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 0.0811 - accuracy: 1.0000\n",
            "Epoch 5/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 0.0356 - accuracy: 1.0000\n",
            "Epoch 6/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 0.0195 - accuracy: 1.0000\n",
            "Epoch 7/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 0.0122 - accuracy: 1.0000\n",
            "Epoch 8/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 0.0083 - accuracy: 1.0000\n",
            "Epoch 9/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 0.0059 - accuracy: 1.0000\n",
            "Epoch 10/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 1.0000\n",
            "Epoch 11/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 12/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 13/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 14/250\n",
            "176/176 [==============================] - 0s 3ms/step - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 15/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 16/250\n",
            "176/176 [==============================] - 0s 3ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 17/250\n",
            "176/176 [==============================] - 0s 3ms/step - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 18/250\n",
            "176/176 [==============================] - 0s 3ms/step - loss: 8.7665e-04 - accuracy: 1.0000\n",
            "Epoch 19/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 7.4920e-04 - accuracy: 1.0000\n",
            "Epoch 20/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 6.4426e-04 - accuracy: 1.0000\n",
            "Epoch 21/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 5.5678e-04 - accuracy: 1.0000\n",
            "Epoch 22/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 4.8345e-04 - accuracy: 1.0000\n",
            "Epoch 23/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 4.2132e-04 - accuracy: 1.0000\n",
            "Epoch 24/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 3.6836e-04 - accuracy: 1.0000\n",
            "Epoch 25/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 3.2318e-04 - accuracy: 1.0000\n",
            "Epoch 26/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 2.8426e-04 - accuracy: 1.0000\n",
            "Epoch 27/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 2.5070e-04 - accuracy: 1.0000\n",
            "Epoch 28/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 2.2155e-04 - accuracy: 1.0000\n",
            "Epoch 29/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.9621e-04 - accuracy: 1.0000\n",
            "Epoch 30/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.7410e-04 - accuracy: 1.0000\n",
            "Epoch 31/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.5473e-04 - accuracy: 1.0000\n",
            "Epoch 32/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.3769e-04 - accuracy: 1.0000\n",
            "Epoch 33/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.2272e-04 - accuracy: 1.0000\n",
            "Epoch 34/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.0955e-04 - accuracy: 1.0000\n",
            "Epoch 35/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 9.7893e-05 - accuracy: 1.0000\n",
            "Epoch 36/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 8.7578e-05 - accuracy: 1.0000\n",
            "Epoch 37/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 7.8414e-05 - accuracy: 1.0000\n",
            "Epoch 38/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 7.0272e-05 - accuracy: 1.0000\n",
            "Epoch 39/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 6.3048e-05 - accuracy: 1.0000\n",
            "Epoch 40/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 5.6596e-05 - accuracy: 1.0000\n",
            "Epoch 41/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 5.0833e-05 - accuracy: 1.0000\n",
            "Epoch 42/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 4.5672e-05 - accuracy: 1.0000\n",
            "Epoch 43/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 4.1088e-05 - accuracy: 1.0000\n",
            "Epoch 44/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 3.6958e-05 - accuracy: 1.0000\n",
            "Epoch 45/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 3.3274e-05 - accuracy: 1.0000\n",
            "Epoch 46/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 2.9974e-05 - accuracy: 1.0000\n",
            "Epoch 47/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 2.7022e-05 - accuracy: 1.0000\n",
            "Epoch 48/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 2.4352e-05 - accuracy: 1.0000\n",
            "Epoch 49/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 2.1966e-05 - accuracy: 1.0000\n",
            "Epoch 50/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.9822e-05 - accuracy: 1.0000\n",
            "Epoch 51/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.7878e-05 - accuracy: 1.0000\n",
            "Epoch 52/250\n",
            "176/176 [==============================] - 0s 3ms/step - loss: 1.6141e-05 - accuracy: 1.0000\n",
            "Epoch 53/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.4585e-05 - accuracy: 1.0000\n",
            "Epoch 54/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.3173e-05 - accuracy: 1.0000\n",
            "Epoch 55/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.1899e-05 - accuracy: 1.0000\n",
            "Epoch 56/250\n",
            "176/176 [==============================] - 0s 3ms/step - loss: 1.0754e-05 - accuracy: 1.0000\n",
            "Epoch 57/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 9.7167e-06 - accuracy: 1.0000\n",
            "Epoch 58/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 8.7828e-06 - accuracy: 1.0000\n",
            "Epoch 59/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 7.9468e-06 - accuracy: 1.0000\n",
            "Epoch 60/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 7.1935e-06 - accuracy: 1.0000\n",
            "Epoch 61/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 6.5035e-06 - accuracy: 1.0000\n",
            "Epoch 62/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 5.8823e-06 - accuracy: 1.0000\n",
            "Epoch 63/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 5.3292e-06 - accuracy: 1.0000\n",
            "Epoch 64/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 4.8202e-06 - accuracy: 1.0000\n",
            "Epoch 65/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 4.3652e-06 - accuracy: 1.0000\n",
            "Epoch 66/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 3.9459e-06 - accuracy: 1.0000\n",
            "Epoch 67/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 3.5700e-06 - accuracy: 1.0000\n",
            "Epoch 68/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 3.2422e-06 - accuracy: 1.0000\n",
            "Epoch 69/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 2.9315e-06 - accuracy: 1.0000\n",
            "Epoch 70/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 2.6561e-06 - accuracy: 1.0000\n",
            "Epoch 71/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 2.4125e-06 - accuracy: 1.0000\n",
            "Epoch 72/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 2.1865e-06 - accuracy: 1.0000\n",
            "Epoch 73/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.9885e-06 - accuracy: 1.0000\n",
            "Epoch 74/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.8037e-06 - accuracy: 1.0000\n",
            "Epoch 75/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.6184e-06 - accuracy: 1.0000\n",
            "Epoch 76/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.4695e-06 - accuracy: 1.0000\n",
            "Epoch 77/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.3307e-06 - accuracy: 1.0000\n",
            "Epoch 78/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.2041e-06 - accuracy: 1.0000\n",
            "Epoch 79/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.1007e-06 - accuracy: 1.0000\n",
            "Epoch 80/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 9.8653e-07 - accuracy: 1.0000\n",
            "Epoch 81/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 8.8677e-07 - accuracy: 1.0000\n",
            "Epoch 82/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 8.1281e-07 - accuracy: 1.0000\n",
            "Epoch 83/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 7.5168e-07 - accuracy: 1.0000\n",
            "Epoch 84/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 6.7416e-07 - accuracy: 1.0000\n",
            "Epoch 85/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 6.2016e-07 - accuracy: 1.0000\n",
            "Epoch 86/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 5.5359e-07 - accuracy: 1.0000\n",
            "Epoch 87/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 4.9187e-07 - accuracy: 1.0000\n",
            "Epoch 88/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 4.6003e-07 - accuracy: 1.0000\n",
            "Epoch 89/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 4.1460e-07 - accuracy: 1.0000\n",
            "Epoch 90/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 3.6383e-07 - accuracy: 1.0000\n",
            "Epoch 91/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 3.2460e-07 - accuracy: 1.0000\n",
            "Epoch 92/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 2.9225e-07 - accuracy: 1.0000\n",
            "Epoch 93/250\n",
            "176/176 [==============================] - 0s 3ms/step - loss: 2.7145e-07 - accuracy: 1.0000\n",
            "Epoch 94/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 2.4309e-07 - accuracy: 1.0000\n",
            "Epoch 95/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 2.1278e-07 - accuracy: 1.0000\n",
            "Epoch 96/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.9350e-07 - accuracy: 1.0000\n",
            "Epoch 97/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.8060e-07 - accuracy: 1.0000\n",
            "Epoch 98/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.5597e-07 - accuracy: 1.0000\n",
            "Epoch 99/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.4562e-07 - accuracy: 1.0000\n",
            "Epoch 100/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.2660e-07 - accuracy: 1.0000\n",
            "Epoch 101/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.1267e-07 - accuracy: 1.0000\n",
            "Epoch 102/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.0061e-07 - accuracy: 1.0000\n",
            "Epoch 103/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 8.6435e-08 - accuracy: 1.0000\n",
            "Epoch 104/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 7.5737e-08 - accuracy: 1.0000\n",
            "Epoch 105/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 7.0643e-08 - accuracy: 1.0000\n",
            "Epoch 106/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 6.1727e-08 - accuracy: 1.0000\n",
            "Epoch 107/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 5.4171e-08 - accuracy: 1.0000\n",
            "Epoch 108/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 4.3812e-08 - accuracy: 1.0000\n",
            "Epoch 109/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 4.0331e-08 - accuracy: 1.0000\n",
            "Epoch 110/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 3.1925e-08 - accuracy: 1.0000\n",
            "Epoch 111/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 2.8783e-08 - accuracy: 1.0000\n",
            "Epoch 112/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 2.4453e-08 - accuracy: 1.0000\n",
            "Epoch 113/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 2.1397e-08 - accuracy: 1.0000\n",
            "Epoch 114/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.8595e-08 - accuracy: 1.0000\n",
            "Epoch 115/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.6812e-08 - accuracy: 1.0000\n",
            "Epoch 116/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.2481e-08 - accuracy: 1.0000\n",
            "Epoch 117/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 8.8303e-09 - accuracy: 1.0000\n",
            "Epoch 118/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 5.6888e-09 - accuracy: 1.0000\n",
            "Epoch 119/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 5.7737e-09 - accuracy: 1.0000\n",
            "Epoch 120/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 6.4529e-09 - accuracy: 1.0000\n",
            "Epoch 121/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 6.2831e-09 - accuracy: 1.0000\n",
            "Epoch 122/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 6.0284e-09 - accuracy: 1.0000\n",
            "Epoch 123/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 5.6039e-09 - accuracy: 1.0000\n",
            "Epoch 124/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 5.5189e-09 - accuracy: 1.0000\n",
            "Epoch 125/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 5.9435e-09 - accuracy: 1.0000\n",
            "Epoch 126/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 5.3491e-09 - accuracy: 1.0000\n",
            "Epoch 127/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 5.3491e-09 - accuracy: 1.0000\n",
            "Epoch 128/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 5.6039e-09 - accuracy: 1.0000\n",
            "Epoch 129/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 5.0095e-09 - accuracy: 1.0000\n",
            "Epoch 130/250\n",
            "176/176 [==============================] - 0s 3ms/step - loss: 5.1793e-09 - accuracy: 1.0000\n",
            "Epoch 131/250\n",
            "176/176 [==============================] - 0s 3ms/step - loss: 5.3491e-09 - accuracy: 1.0000\n",
            "Epoch 132/250\n",
            "176/176 [==============================] - 0s 3ms/step - loss: 4.9246e-09 - accuracy: 1.0000\n",
            "Epoch 133/250\n",
            "176/176 [==============================] - 0s 3ms/step - loss: 5.7737e-09 - accuracy: 1.0000\n",
            "Epoch 134/250\n",
            "176/176 [==============================] - 1s 5ms/step - loss: 4.7548e-09 - accuracy: 1.0000\n",
            "Epoch 135/250\n",
            "176/176 [==============================] - 1s 3ms/step - loss: 5.2642e-09 - accuracy: 1.0000\n",
            "Epoch 136/250\n",
            "176/176 [==============================] - 1s 3ms/step - loss: 5.3491e-09 - accuracy: 1.0000\n",
            "Epoch 137/250\n",
            "176/176 [==============================] - 0s 3ms/step - loss: 4.6699e-09 - accuracy: 1.0000\n",
            "Epoch 138/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 5.1793e-09 - accuracy: 1.0000\n",
            "Epoch 139/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 4.8397e-09 - accuracy: 1.0000\n",
            "Epoch 140/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 4.6699e-09 - accuracy: 1.0000\n",
            "Epoch 141/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 4.7548e-09 - accuracy: 1.0000\n",
            "Epoch 142/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 4.5001e-09 - accuracy: 1.0000\n",
            "Epoch 143/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 4.6699e-09 - accuracy: 1.0000\n",
            "Epoch 144/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 4.6699e-09 - accuracy: 1.0000\n",
            "Epoch 145/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 3.8208e-09 - accuracy: 1.0000\n",
            "Epoch 146/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 4.2453e-09 - accuracy: 1.0000\n",
            "Epoch 147/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 4.0755e-09 - accuracy: 1.0000\n",
            "Epoch 148/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 4.3303e-09 - accuracy: 1.0000\n",
            "Epoch 149/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 3.8208e-09 - accuracy: 1.0000\n",
            "Epoch 150/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 3.9906e-09 - accuracy: 1.0000\n",
            "Epoch 151/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 3.9057e-09 - accuracy: 1.0000\n",
            "Epoch 152/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 3.3114e-09 - accuracy: 1.0000\n",
            "Epoch 153/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 3.9057e-09 - accuracy: 1.0000\n",
            "Epoch 154/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 3.9906e-09 - accuracy: 1.0000\n",
            "Epoch 155/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 3.9057e-09 - accuracy: 1.0000\n",
            "Epoch 156/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 3.7359e-09 - accuracy: 1.0000\n",
            "Epoch 157/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 4.0755e-09 - accuracy: 1.0000\n",
            "Epoch 158/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 3.9057e-09 - accuracy: 1.0000\n",
            "Epoch 159/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 3.8208e-09 - accuracy: 1.0000\n",
            "Epoch 160/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 4.3303e-09 - accuracy: 1.0000\n",
            "Epoch 161/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 4.7548e-09 - accuracy: 1.0000\n",
            "Epoch 162/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 4.4152e-09 - accuracy: 1.0000\n",
            "Epoch 163/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 5.1793e-09 - accuracy: 1.0000\n",
            "Epoch 164/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 5.1793e-09 - accuracy: 1.0000\n",
            "Epoch 165/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 5.5189e-09 - accuracy: 1.0000\n",
            "Epoch 166/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 5.0944e-09 - accuracy: 1.0000\n",
            "Epoch 167/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 5.6039e-09 - accuracy: 1.0000\n",
            "Epoch 168/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 5.0095e-09 - accuracy: 1.0000\n",
            "Epoch 169/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 6.3680e-09 - accuracy: 1.0000\n",
            "Epoch 170/250\n",
            "176/176 [==============================] - 0s 3ms/step - loss: 3.9057e-09 - accuracy: 1.0000\n",
            "Epoch 171/250\n",
            "176/176 [==============================] - 0s 3ms/step - loss: 5.2642e-09 - accuracy: 1.0000\n",
            "Epoch 172/250\n",
            "176/176 [==============================] - 1s 3ms/step - loss: 4.8397e-09 - accuracy: 1.0000\n",
            "Epoch 173/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 3.5661e-09 - accuracy: 1.0000\n",
            "Epoch 174/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 2.9717e-09 - accuracy: 1.0000\n",
            "Epoch 175/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 4.0755e-09 - accuracy: 1.0000\n",
            "Epoch 176/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 3.2265e-09 - accuracy: 1.0000\n",
            "Epoch 177/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 4.1604e-09 - accuracy: 1.0000\n",
            "Epoch 178/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 5.6888e-09 - accuracy: 1.0000\n",
            "Epoch 179/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 3.3963e-09 - accuracy: 1.0000\n",
            "Epoch 180/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 3.9057e-09 - accuracy: 1.0000\n",
            "Epoch 181/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 2.2840e-08 - accuracy: 1.0000\n",
            "Epoch 182/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 8.4907e-11 - accuracy: 1.0000\n",
            "Epoch 183/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 2.5472e-10 - accuracy: 1.0000\n",
            "Epoch 184/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 5.3491e-09 - accuracy: 1.0000\n",
            "Epoch 185/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 3.3963e-09 - accuracy: 1.0000\n",
            "Epoch 186/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 0.0139 - accuracy: 0.9950\n",
            "Epoch 187/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 0.9986\n",
            "Epoch 188/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 2.5839e-06 - accuracy: 1.0000\n",
            "Epoch 189/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 2.1512e-06 - accuracy: 1.0000\n",
            "Epoch 190/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.8236e-06 - accuracy: 1.0000\n",
            "Epoch 191/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.5735e-06 - accuracy: 1.0000\n",
            "Epoch 192/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.3614e-06 - accuracy: 1.0000\n",
            "Epoch 193/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.1847e-06 - accuracy: 1.0000\n",
            "Epoch 194/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.0368e-06 - accuracy: 1.0000\n",
            "Epoch 195/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 9.1487e-07 - accuracy: 1.0000\n",
            "Epoch 196/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 8.1018e-07 - accuracy: 1.0000\n",
            "Epoch 197/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 7.1789e-07 - accuracy: 1.0000\n",
            "Epoch 198/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 6.4122e-07 - accuracy: 1.0000\n",
            "Epoch 199/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 5.7108e-07 - accuracy: 1.0000\n",
            "Epoch 200/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 5.1403e-07 - accuracy: 1.0000\n",
            "Epoch 201/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 4.6249e-07 - accuracy: 1.0000\n",
            "Epoch 202/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 4.1834e-07 - accuracy: 1.0000\n",
            "Epoch 203/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 3.7656e-07 - accuracy: 1.0000\n",
            "Epoch 204/250\n",
            "176/176 [==============================] - 0s 3ms/step - loss: 3.3895e-07 - accuracy: 1.0000\n",
            "Epoch 205/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 3.0940e-07 - accuracy: 1.0000\n",
            "Epoch 206/250\n",
            "176/176 [==============================] - 0s 3ms/step - loss: 2.8147e-07 - accuracy: 1.0000\n",
            "Epoch 207/250\n",
            "176/176 [==============================] - 0s 3ms/step - loss: 2.5489e-07 - accuracy: 1.0000\n",
            "Epoch 208/250\n",
            "176/176 [==============================] - 0s 3ms/step - loss: 2.2721e-07 - accuracy: 1.0000\n",
            "Epoch 209/250\n",
            "176/176 [==============================] - 0s 3ms/step - loss: 2.0547e-07 - accuracy: 1.0000\n",
            "Epoch 210/250\n",
            "176/176 [==============================] - 0s 3ms/step - loss: 1.8459e-07 - accuracy: 1.0000\n",
            "Epoch 211/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.7015e-07 - accuracy: 1.0000\n",
            "Epoch 212/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.5750e-07 - accuracy: 1.0000\n",
            "Epoch 213/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.4307e-07 - accuracy: 1.0000\n",
            "Epoch 214/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.3330e-07 - accuracy: 1.0000\n",
            "Epoch 215/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.2328e-07 - accuracy: 1.0000\n",
            "Epoch 216/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.1556e-07 - accuracy: 1.0000\n",
            "Epoch 217/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.0545e-07 - accuracy: 1.0000\n",
            "Epoch 218/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.0206e-07 - accuracy: 1.0000\n",
            "Epoch 219/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 9.5266e-08 - accuracy: 1.0000\n",
            "Epoch 220/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 8.5671e-08 - accuracy: 1.0000\n",
            "Epoch 221/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 7.9897e-08 - accuracy: 1.0000\n",
            "Epoch 222/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 7.3954e-08 - accuracy: 1.0000\n",
            "Epoch 223/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 6.7076e-08 - accuracy: 1.0000\n",
            "Epoch 224/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 6.3510e-08 - accuracy: 1.0000\n",
            "Epoch 225/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 6.0708e-08 - accuracy: 1.0000\n",
            "Epoch 226/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 5.8076e-08 - accuracy: 1.0000\n",
            "Epoch 227/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 5.3831e-08 - accuracy: 1.0000\n",
            "Epoch 228/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 5.0944e-08 - accuracy: 1.0000\n",
            "Epoch 229/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 4.5595e-08 - accuracy: 1.0000\n",
            "Epoch 230/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 4.3642e-08 - accuracy: 1.0000\n",
            "Epoch 231/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 3.9312e-08 - accuracy: 1.0000\n",
            "Epoch 232/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 3.4727e-08 - accuracy: 1.0000\n",
            "Epoch 233/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 3.1076e-08 - accuracy: 1.0000\n",
            "Epoch 234/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 3.0312e-08 - accuracy: 1.0000\n",
            "Epoch 235/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 2.8274e-08 - accuracy: 1.0000\n",
            "Epoch 236/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 2.5302e-08 - accuracy: 1.0000\n",
            "Epoch 237/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 2.2840e-08 - accuracy: 1.0000\n",
            "Epoch 238/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 2.0293e-08 - accuracy: 1.0000\n",
            "Epoch 239/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.5962e-08 - accuracy: 1.0000\n",
            "Epoch 240/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.5368e-08 - accuracy: 1.0000\n",
            "Epoch 241/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.3500e-08 - accuracy: 1.0000\n",
            "Epoch 242/250\n",
            "176/176 [==============================] - 0s 3ms/step - loss: 1.1038e-08 - accuracy: 1.0000\n",
            "Epoch 243/250\n",
            "176/176 [==============================] - 0s 3ms/step - loss: 1.0783e-08 - accuracy: 1.0000\n",
            "Epoch 244/250\n",
            "176/176 [==============================] - 0s 3ms/step - loss: 1.1293e-08 - accuracy: 1.0000\n",
            "Epoch 245/250\n",
            "176/176 [==============================] - 0s 3ms/step - loss: 1.1972e-08 - accuracy: 1.0000\n",
            "Epoch 246/250\n",
            "176/176 [==============================] - 0s 3ms/step - loss: 1.1462e-08 - accuracy: 1.0000\n",
            "Epoch 247/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 1.1717e-08 - accuracy: 1.0000\n",
            "Epoch 248/250\n",
            "176/176 [==============================] - 0s 3ms/step - loss: 1.1462e-08 - accuracy: 1.0000\n",
            "Epoch 249/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 7.6416e-09 - accuracy: 1.0000\n",
            "Epoch 250/250\n",
            "176/176 [==============================] - 0s 2ms/step - loss: 5.3491e-09 - accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "pickle.dump( {'words':words, 'classes':classes}, open( \"training_data\", \"wb\" ))"
      ],
      "metadata": {
        "id": "Z0F1befAxZeQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "model = load_model(\"model.pk1\")"
      ],
      "metadata": {
        "id": "XVKB6vw37Ipk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# restoring all the data structures\n",
        "data = pickle.load( open( \"training_data\", \"rb\"))\n",
        "woords = data['words']\n",
        "classes = data['classes']"
      ],
      "metadata": {
        "id": "O_Ymsuev7Vos"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('intents.json') as json_data:\n",
        "  intents = json.load(json_data)"
      ],
      "metadata": {
        "id": "wwFkoaa67puJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_up_sentence(sentence):\n",
        "  #tokenizing the pattern\n",
        "  sentence_word = nltk.word_tokenize(sentence)\n",
        "  #stemming each word\n",
        "  sentence_word = [stemmer.stem(word.lower()) for word in sentence_word]\n",
        "  return sentence_word\n",
        "\n",
        "# returning bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
        "def bow(sentence, words):\n",
        "  # tokenizing the pattern\n",
        "  sentence_words = clean_up_sentence(sentence)\n",
        "  # generating bag of words\n",
        "  bag = [0]*len(words)\n",
        "  for s in sentence_words:\n",
        "    for i,w in enumerate(words):\n",
        "      if w == s:\n",
        "        bag[i] = 1\n",
        "  bag = np.array(bag)\n",
        "  return(bag)"
      ],
      "metadata": {
        "id": "MwJ0wOb570ez"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops.gen_data_flow_ops import resource_accumulator_set_global_step\n",
        "ERROR_THRESHOLD = 0.30\n",
        "def classify(sentence):\n",
        "  # generate probablities from the model\n",
        "  bag = bow(sentence, words)\n",
        "  results = model.predict(np.array([bag]))\n",
        "  # filter out prediction below a threshold\n",
        "  results = [[i,r] for i,r in enumerate(results[0]) if r> ERROR_THRESHOLD]\n",
        "  # sort by strength of probablity\n",
        "  results.sort(key=lambda x: x[1], reverse=True)\n",
        "  return_list = []\n",
        "  for r in results:\n",
        "    return_list.append((classes[r[0]], r[1]))\n",
        "  # return tuple of intent and probability\n",
        "  return return_list\n",
        "\n",
        "def response(sentence):\n",
        "  results = classify(sentence)\n",
        "  # if we have a classification then find the matching intent tag\n",
        "  if results:\n",
        "    # loop as long as there are matches to process\n",
        "    while resource_accumulator_set_global_step:\n",
        "      for i in intents['intents']:\n",
        "        # find a tag matching the first result\n",
        "        if i['tag'] == results[0][0]:\n",
        "          # a random response from the intent\n",
        "          return print(random.choice(i['responses']))\n",
        "\n",
        "      results.pop(0)\n"
      ],
      "metadata": {
        "id": "fwcEGpBq_jgu"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response('Where are you located?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocEoNUalCMHz",
        "outputId": "40648f5d-7a5d-44bc-b5f8-e391c7a19976"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 88ms/step\n",
            "Thans for your Interest. I live in India.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response('hi how are you?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0GI1w9-CeOL",
        "outputId": "12b45325-a376-4a1c-c784-d4da24f4de24"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 26ms/step\n",
            "Hi there, how can I help?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1xmFi8trDBXS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}